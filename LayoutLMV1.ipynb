{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "LayoutLMV1.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4jq1qE4bba1",
    "outputId": "b2f89602-b1eb-4f18-ceae-7d0684035a1c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n",
      "--2021-12-08 23:48:17--  https://storage.googleapis.com/openimages/challenge_2019/challenge-2019-train-detection-bbox.csv\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.210.128, 173.194.213.128, 173.194.215.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.210.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 996981249 (951M) [application/octet-stream]\n",
      "Saving to: ‘/content/data/challenge-2019-train-detection-bbox.csv’\n",
      "\n",
      "challenge-2019-trai 100%[===================>] 950.79M   167MB/s    in 5.8s    \n",
      "\n",
      "2021-12-08 23:48:23 (165 MB/s) - ‘/content/data/challenge-2019-train-detection-bbox.csv’ saved [996981249/996981249]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!wget -P /content/data/ https://storage.googleapis.com/openimages/challenge_2019/challenge-2019-train-detection-bbox.csv\n",
    "!wget -P /content/data/ https://storage.googleapis.com/openimages/challenge_2019/challenge-2019-train-detection-human-imagelabels.csv"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install aws-shell"
   ],
   "metadata": {
    "id": "RanCujgvUI_W"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!aws s3 --no-sign-request cp s3://open-images-dataset/tar/train_7.tar.gz /content/data/images/"
   ],
   "metadata": {
    "id": "lCO9HDoGUMt7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!aws s3 --no-sign-request cp s3://open-images-dataset/tar/train_7.tar.gz /content/data/images/\n",
    "!tar -xvzf /content/data/images/train_7.tar.gz -C /content/data/images/"
   ],
   "metadata": {
    "id": "L5gMWYmiUXhS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "FUNSD Dataset"
   ],
   "metadata": {
    "id": "1un2MoninGhL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!wget https://guillaumejaume.github.io/FUNSD/dataset.zip /content/\n",
    "!unzip dataset.zip -d /content/\n",
    "\n",
    "!cp -r /content/drive//My\\ Drive/GoogleColab/Personal/Project/open_image_challenge/ open_image_challenge/"
   ],
   "metadata": {
    "id": "hT2GqjGvVH3Y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%cd /content/open_image_challenge/"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VqI1APuLKLER",
    "outputId": "6b1e9796-2669-4ad6-e542-35a8708980ac"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/open_image_challenge\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Restart the kernel after running this cell\n",
    "%%capture\n",
    "!pip install -r requirements.txt"
   ],
   "metadata": {
    "id": "VmLDrD1CKQbd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os.path\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from DataPrepararation import DataPreparation\n",
    "from FUNSDDataset import FUNSDDataset\n",
    "from RCNN import FasterRCNNModel\n",
    "from LayoutLM import LayoutLMImageEmbeddingModel\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD, AdamW"
   ],
   "metadata": {
    "id": "Np4InnwyofN7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data_folder = '/content/dataset/'\n",
    "\n",
    "dataprep = DataPreparation(data_folder=data_folder)\n",
    "df_train, df_test = dataprep.prepare_dataframe()"
   ],
   "metadata": {
    "id": "0reObzjgILN9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_train_m, df_train_p = train_test_split(df_train, train_size=0.66667)\n",
    "df_train_m = df_train_m.reset_index(drop=True)\n",
    "df_train_p = df_train_p.reset_index(drop=True)"
   ],
   "metadata": {
    "id": "ThYwZX_SpgKX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset_m = FUNSDDataset(\n",
    "    image_data_folder=os.path.join(data_folder, 'training_data/images'),\n",
    "    input_ids=df_train_m['input_ids'],\n",
    "    token_type_ids=df_train_m['token_type_ids'],\n",
    "    attention_mask=df_train_m['attention_mask'],\n",
    "    imageid=df_train_m['imageid'],\n",
    "    bbox=df_train_m['bboxes'],\n",
    "    image_size=512,\n",
    "    labels=df_train_m['labels']\n",
    ")\n",
    "\n",
    "train_dataset_p = FUNSDDataset(\n",
    "    image_data_folder=os.path.join(data_folder, 'training_data/images'),\n",
    "    input_ids=df_train_p['input_ids'],\n",
    "    token_type_ids=df_train_p['token_type_ids'],\n",
    "    attention_mask=df_train_p['attention_mask'],\n",
    "    imageid=df_train_p['imageid'],\n",
    "    bbox=df_train_p['bboxes'],\n",
    "    image_size=512,\n",
    "    labels=df_train_p['labels']\n",
    ")\n",
    "\n",
    "test_dataset = FUNSDDataset(\n",
    "    image_data_folder=os.path.join(data_folder, 'testing_data/images'),\n",
    "    input_ids=df_test['input_ids'],\n",
    "    token_type_ids=df_test['token_type_ids'],\n",
    "    attention_mask=df_test['attention_mask'],\n",
    "    imageid=df_test['imageid'],\n",
    "    bbox=df_test['bboxes'],\n",
    "    image_size=224,\n",
    "    labels=df_test['labels']\n",
    ")\n",
    "\n",
    "\n",
    "train_loader_m = DataLoader(train_dataset_m, batch_size=2)\n",
    "train_loader_p = DataLoader(train_dataset_p, batch_size=1)\n",
    "test_loader = DataLoader(test_dataset)"
   ],
   "metadata": {
    "id": "tVNJx43uIY3E"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "rcnn_model = FasterRCNNModel(num_classes=5)\n",
    "optim_rcnn = SGD(rcnn_model.parameters(), lr=5e-3, momentum=0.9, weight_decay=0.0005)"
   ],
   "metadata": {
    "id": "NEPDfs-SsRgO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "device_ = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "rcnn_model.train()\n",
    "rcnn_model.to(device_)\n",
    "losses = []\n",
    "\n",
    "epochs = 4\n",
    "for epoch in range(epochs):\n",
    "    print('-------------------------------------')\n",
    "    print(f\"Epoch:{epoch + 1}\")\n",
    "    losses.clear()\n",
    "    for batch in train_loader_p:\n",
    "        images = batch['images'].to(device_)\n",
    "        orig_len = batch['orig_len']\n",
    "        boxes = batch['bbox_rcnn']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        targets = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "        images = list(image for image in images)\n",
    "        targets = [{k: v[b][:orig_len[b]].to(device_) for k, v in targets.items()} for b in range(len(images))]\n",
    "\n",
    "        optim_rcnn.zero_grad()\n",
    "        output_losses = rcnn_model(images, targets)\n",
    "        optim_rcnn.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss = sum([output_losses[k].item() for k in output_losses.keys()])\n",
    "            losses.append(copy.deepcopy(loss))\n",
    "\n",
    "    print(\"Loss:\", sum(losses)/len(losses))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDMawzaTIcEj",
    "outputId": "cd57257d-9b6e-45cf-e733-e72cd8d68c4a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------------------------------\n",
      "Epoch:1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 3.1010400993889196\n",
      "-------------------------------------\n",
      "Epoch:2\n",
      "Loss: 3.0981275744503365\n",
      "-------------------------------------\n",
      "Epoch:3\n",
      "Loss: 3.1004921312397347\n",
      "-------------------------------------\n",
      "Epoch:4\n",
      "Loss: 3.0972614247864114\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "del losses"
   ],
   "metadata": {
    "id": "p4ggXVNU83VY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "layoutlm_model = LayoutLMImageEmbeddingModel(num_classes=5, pretrained_rcnn_model=rcnn_model)\n",
    "optim_layoutlm = AdamW(layoutlm_model.parameters(), lr=5e-5)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hv5h2aEwsUbn",
    "outputId": "bdb404f4-fa35-47d6-e484-eb8bb73b5870"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at microsoft/layoutlm-base-uncased were not used when initializing LayoutLMModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing LayoutLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LayoutLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "layoutlm_model.to(device_)\n",
    "layoutlm_model.train()\n",
    "\n",
    "epochs = 4\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    print('-------------------------------------')\n",
    "    print(f\"Epoch:{epoch + 1}\")\n",
    "    losses.clear()\n",
    "\n",
    "    for batch in train_loader_m:\n",
    "        input_ids = batch['input_ids'].to(device_)\n",
    "        attention_mask = batch['attention_mask'].to(device_)\n",
    "        images = batch['images'].to(device_)\n",
    "        bbox_layoutlm = batch['bbox_layoutlm'].to(device_)\n",
    "        bbox_rcnn = batch['bbox_rcnn'].to(device_)\n",
    "        labels = batch['labels'].to(device_)\n",
    "\n",
    "        output = layoutlm_model(input_ids, bbox_layoutlm, bbox_rcnn, images, attention_mask, labels)\n",
    "\n",
    "        optim_layoutlm.zero_grad()\n",
    "        output.loss.backward()\n",
    "        optim_layoutlm.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            losses.append(copy.deepcopy(output.loss.item()))\n",
    "\n",
    "    print(\"Loss:\", sum(losses)/len(losses))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MdgiwN__rggU",
    "outputId": "3c34a16d-01b9-4c92-c781-649c2134283a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------------------------------\n",
      "Epoch:1\n",
      "Loss: 0.5779041320085525\n",
      "-------------------------------------\n",
      "Epoch:2\n",
      "Loss: 0.30978871643543243\n",
      "-------------------------------------\n",
      "Epoch:3\n",
      "Loss: 0.2594358536601067\n",
      "-------------------------------------\n",
      "Epoch:4\n",
      "Loss: 0.19255270302295685\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "layoutlm_model.eval()\n",
    "\n",
    "y_preds = []\n",
    "y_true = []\n",
    "for batch in test_loader:\n",
    "    input_ids = batch['input_ids'].to(device_)\n",
    "    attention_mask = batch['attention_mask'].to(device_)\n",
    "    images = batch['images'].to(device_)\n",
    "    bbox_layoutlm = batch['bbox_layoutlm'].to(device_)\n",
    "    bbox_rcnn = batch['bbox_rcnn'].to(device_)\n",
    "    labels = batch['labels']\n",
    "\n",
    "    output = layoutlm_model(input_ids, bbox_layoutlm, bbox_rcnn, images, attention_mask)\n",
    "    \n",
    "    pred = torch.argmax(output.logits, dim=-1)\n",
    "    y_preds.append(pred.cpu().tolist())\n",
    "    y_true.append(labels.cpu().tolist())\n"
   ],
   "metadata": {
    "id": "uevlAj6E8NE1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "accuracy = torch.sum(y_true_tensor == y_pred_tensor) / torch.numel(y_true_tensor)\n",
    "print(accuracy)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQ9sA3UA-kSY",
    "outputId": "8712ec5b-76a8-46e2-ed42-f7200317cd93"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.9436)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "vWp0rQmqDGam"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}